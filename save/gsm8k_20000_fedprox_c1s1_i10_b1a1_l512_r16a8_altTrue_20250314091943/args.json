{
    "script_args": {
        "model_name_or_path": "meta-llama/Llama-2-7b-hf",
        "dataset_name": "gsm8k",
        "log_with": "none",
        "learning_rate": 2e-05,
        "batch_size": 1,
        "seq_length": 512,
        "gradient_accumulation_steps": 1,
        "load_in_8bit": false,
        "load_in_4bit": true,
        "use_peft": true,
        "trust_remote_code": false,
        "output_dir": "save/gsm8k_20000_fedprox_c1s1_i10_b1a1_l512_r16a8_altTrue_20250314091943",
        "peft_lora_r": 16,
        "peft_lora_alpha": 8,
        "logging_steps": 100,
        "use_auth_token": false,
        "num_train_epochs": 3,
        "max_steps": 10,
        "save_steps": 1000,
        "save_total_limit": 10,
        "push_to_hub": false,
        "hub_model_id": null,
        "gradient_checkpointing": true,
        "optim": "sgd",
        "template": "alpaca",
        "seed": 2023,
        "dpo_beta": 0.1,
        "dataset_sample": 20000,
        "local_data_dir": null,
        "alt_opt": true
    },
    "fed_args": {
        "fed_alg": "fedprox",
        "num_rounds": 200,
        "num_clients": 1,
        "sample_clients": 1,
        "split_strategy": "iid",
        "prox_mu": 0.001,
        "fedopt_tau": 0.001,
        "fedopt_eta": 0.001,
        "fedopt_beta1": 0.9,
        "fedopt_beta2": 0.99,
        "save_model_freq": 200
    }
}